# coding=utf-8
# Copyright 2018 The Google AI Language Team Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Tokenization classes."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import re
import unicodedata
import six
import tensorflow as tf


def validate_case_matches_checkpoint(do_lower_case, init_checkpoint):
  """Checks whether the casing config is consistent with the checkpoint name."""

  # The casing has to be passed in by the user and there is no explicit check
  # as to whether it matches the checkpoint. The casing information probably
  # should have been stored in the bert_config.json file, but it's not, so
  # we have to heuristically detect it to validate.

  if not init_checkpoint:
    return

  m = re.match("^.*?([A-Za-z0-9_-]+)/bert_model.ckpt", init_checkpoint)
  if m is None:
    return

  model_name = m.group(1)

  lower_models = [
      "uncased_L-24_H-1024_A-16", "uncased_L-12_H-768_A-12",
      "multilingual_L-12_H-768_A-12", "chinese_L-12_H-768_A-12"
  ]

  cased_models = [
      "cased_L-12_H-768_A-12", "cased_L-24_H-1024_A-16",
      "multi_cased_L-12_H-768_A-12"
  ]

  is_bad_config = False
  if model_name in lower_models and not do_lower_case:
    is_bad_config = True
    actual_flag = "False"
    case_name = "lowercased"
    opposite_flag = "True"

  if model_name in cased_models and do_lower_case:
    is_bad_config = True
    actual_flag = "True"
    case_name = "cased"
    opposite_flag = "False"

  if is_bad_config:
    raise ValueError(
        "You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. "
        "However, `%s` seems to be a %s model, so you "
        "should pass in `--do_lower_case=%s` so that the fine-tuning matches "
        "how the model was pre-training. If this error is wrong, please "
        "just comment out this check." % (actual_flag, init_checkpoint,
                                          model_name, case_name, opposite_flag))


def convert_to_unicode(text):
  """Converts `text` to Unicode (if it's not already), assuming utf-8 input."""
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode("utf-8", "ignore")
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text.decode("utf-8", "ignore")
    elif isinstance(text, unicode):
      return text
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  else:
    raise ValueError("Not running on Python2 or Python 3?")


def printable_text(text):
  """Returns text encoded in a way suitable for print or `tf.logging`."""

  # These functions want `str` for both Python2 and Python3, but in one case
  # it's a Unicode string and in the other it's a byte string.
  if six.PY3:
    if isinstance(text, str):
      return text
    elif isinstance(text, bytes):
      return text.decode("utf-8", "ignore")
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  elif six.PY2:
    if isinstance(text, str):
      return text
    elif isinstance(text, unicode):
      return text.encode("utf-8")
    else:
      raise ValueError("Unsupported string type: %s" % (type(text)))
  else:
    raise ValueError("Not running on Python2 or Python 3?")


def load_vocab(vocab_file):
  """Loads a vocabulary file into a dictionary."""
  vocab = collections.OrderedDict()
  index = 0
  with tf.gfile.GFile(vocab_file, "r") as reader:
    while True:
      token = convert_to_unicode(reader.readline())
      if not token:
        break
      token = token.strip()
      vocab[token] = index
      index += 1
    print(len(vocab))
    # letter_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G','H', 'I', 'J', 'K', 'L', 'M', 'N',
    #                'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'è¦ˆ', '/', '-', '(', ')','%', 'ã€', 'ä¸¨', 'ã€‘',
    #                'å¿”', 'Ç', '\ue035', 'ï¼©', 'ç—¦', 'å½³', 'æº¦', 'é˜„', 'â€™', '\ue310', 'â–Š', 'äº¸', 'ï¼¥', 'ï¼¡', '\ue220', 'ç', 'æ—‚',
    #                'Ã©', 'åš¯', 'â”‡', 'æ§Š', 'èŒ•', 'åŠ¼', 'æ¶œ', 'é™', 'äº»', 'é¾‰', 'è•', 'Ä', 'æ—³', 'â’', 'å“ƒ', 'ç˜œ', 'æ®„', 'å¸€', 'â…¡',
    #                '\ue222', '`', 'ï¼²', 'è®', 'é•š', 'éƒ—', 'å§‡', 'é¢', 'æ³º', 'å', 'çƒ', 'ç–„', 'âˆ«', 'ç‘§', 'â€', 'å’', 'é–®', '\ue21e',
    #                '\ue41f', 'äº“', 'æ¡­', 'â€•', 'ç½', 'ï¼¤', 'æ¯–', 'è·¬', 'âˆ¨', 'å±¼', '\ue13d', 'é¸', 'æ¢¿', 'èœ', 'èµŸ', '\ue231', 'äº',
    #                'æ›', 'ç¬«', 'ç€µ', 'â’', 'â€¦', 'Ä“', 'æ¤´', 'ï¼¦', 'å‘­', '\ue21d', '\ue317', '\ue221', 'çŠ‡', 'å™ƒ', 'é£¡', 'èƒ', 'å•',
    #                'æ®š', 'â€˜', 'éŠ', 'é”›', 'æˆ', '\ue253', 'æ­€', 'Î™', 'ä½º', 'Ã¨', '\ue32f', 'ç¼', 'å¼¢', 'ï¼®', 'æ°º', 'è½«', 'å²¿',
    #                '\ue40a', 'è¾”', 'é•', '\ue345', 'å‘º', 'éœ', 'é’', 'â’', 'èŒ†', 'ç¶‰', 'å‹®', '\ue21c', 'è³', '\ue627', 'é‘±', 'ç¬ª',
    #                'å—®', 'ç¥¡', '\ue21f', 'â’Œ', '\ue66d', '\ue11d', 'â’', 'â†™', 'é“—', 'é¾ƒ', 'â€“', 'Ãº', 'ãˆ ', 'è® ', 'éƒ¯', 'ç¥¹', 'ç°°',
    #                'å°“', 'èš¨', 'â€', 'é›¬', 'â€œ', 'â€”', 'é‘·', 'é‰',
    #
    #                'ğŸ', 'â', 'ğŸ‡¼', 'ğŸ»', 'å±ƒ', 'ï§ ', 'â·', 'ğŸ‘¼', 'ğŸ¥‚', 'Û¶', 'ï¼«', 'æµ¥', 'ï¸', 'ğŸ’›', 'ğŸ“¢', 'ğŸ™Œ', 'ğŸ’—', 'â',
    #                'â™', 'ğŸ’', 'â£', 'ğŸ“Š', 'ğ™', 'è‘', 'âœ³', 'ğŸ', 'ï¸', 'æ„ ', 'âš¡', 'çŠ´', 'ğŸ†˜', 'ğŸ”', 'é««', 'è°¡', 'ğ™š', 'ğŸ›',
    #                'ğŸ”š', 'æ´', 'é²®', 'åº’', '\ue04f', 'Ã', 'æ­', 'ğŸ’±', 'ğŸˆ·', 'æ’º', 'ğŸ‡µ', 'æˆ‹', 'ğŸˆµ', 'ï¼³', 'æ¤±', 'ğŸŒ»', 'âš ', 'ğŸ—',
    #                'æ­ƒ', 'â™’', 'ğŸ‘‡', 'æµ', 'æ±¯', 'â»', 'ğŸ¤', 'ğŸ', 'â‡', 'â„‘', 'ğŸ‡»', 'â½', 'ğŸ‘Š', 'â¾¦', 'ç•º', 'Î¦', 'ğ€', 'ğŒ', 'æ˜°',
    #                'å—Œ', 'ç§•', 'â™¨', 'ğŸ‡¦', 'è¤´', 'ç¿€', 'é«„', 'ğŸŒ¹', 'â¹', 'âœŒ', 'ğŸ½', 'ğŸ¦', 'â‰', 'å´ƒ', 'ğŸˆ¶', 'ğŸ’', 'ğŸŒ²', 'â¢', 'âœ§',
    #                'ğŸ˜˜', 'å•', 'ï¼µ', 'æ±­', 'ğŸŒ´', 'æš', 'å¶¶', 'ğŸŒ‡', 'ğŸ”‘', 'ğŸ', 'â›', 'åŠ–', 'ğŸ…', 'è­', 'é­‰', 'ğŸ‡', 'â…¥', 'ãŠ—', 'è«¨',
    #                'ğ™§', 'é”', 'ğŸ€', 'ğŸ”†', 'é˜“', 'ğŸ™…', '\ue312', 'æ²', 'â… ', 'â¾¼', 'ğŸ‘‘', 'è°', 'æ›Œ', 'ç·ˆ', 'â›', 'ğŸŒ…', 'ğŸ’¯',
    #                '\ue18d', 'ğŸ‡ª', '\ue796', 'ğŸ”', 'í•˜', 'â™»', 'Ã‹', 'ğŸ‡¸', 'ğŸ™ƒ', 'â¼€', 'â‘¬', 'Å‚', 'æ›©', 'çš', 'Å', 'ğŸ“‰',
    #                '\ue779', 'ğŸ±', 'ğŸ”“', 'åª–', 'ğŸ‡«', 'ç•¾', 'ğŸ’', 'ï¼­', 'å’‘', 'ğŸ°', 'ğŸ‡²', 'ãŠŠ', 'ğŸ¤—', 'ğŸ“Œ', 'å©‚', 'â‚³',
    #                '\ue12f', 'æ‰Œ', 'ğŸŒŠ', 'ğŸ‘†', 'é„ ', 'ğŸ’š', 'ğŸŒ¸', 'â½…', 'éŸ«', 'æ›', 'é‡†', 'ğŸ‡¬', 'â‘ª', 'ğŸ ', 'â“¿', 'å ', 'å›µ', 'ğŸ‡¨',
    #                'ğŸ™', 'éµ', 'â…¹', 'ğŸ‡°', 'ğŸ’¹', 'ğŸŒ³', 'é¸ƒ', 'æ¯‘', 'âˆ·', '\ue41d', 'ï¼¸', 'é‹†', 'Ù©', 'ğŸ‘»', 'å½½', 'ğŸ©', 'æº†', 'â½',
    #                'ğŸ‡·', 'èš°', 'ğŸ‹', 'âœŠ', 'å¡±', 'ç‹²', 'â€¼', 'Ãª', 'ğŸ˜', 'ì˜¤', 'ğŸ’¬', 'ï¹Œ', 'ğ„', 'ğŸŒš', 'é¸±', 'è­', 'ğŸŒ', 'ï¼°', 'é“–',
    #                'â¼¤', 'ğŸ˜±', 'ğŸ‡¾', 'ğŸ˜€', 'ğŸ›¡', 'è¨¾', '\ue021', 'ğŸ˜‰', 'ä¾ª', 'â—€', 'ğŸ‡±', 'ğŸ”±', 'ğ’', 'ğŸ®', 'åœª', 'è¯¨', 'ğŸ’ª',
    #                'â¶', 'ë¯¼', 'æµ¡', 'ğŸ˜', 'ğŸ‡¹', 'ğŸŒ', 'ğŸŒ±', 'é¹†', 'ãŠ™', 'âœ’', 'â°', 'ç¬§', 'ğŸ’¥', 'ğ', 'â˜', 'çŒŠ', 'ğŸ”…', 'ğŸ”’',
    #                'ğŸ”›', 'ğŸ‰', 'é”', 'é‚¡', 'ğŸ’¡', 'â¬‡', 'æ¸½', 'ğŸš©', 'é­‘', 'ğŸ’', '\ue04c', '\ue307', 'é¸©', 'ğŸ‡®', 'ğŸ»', 'ğŸŒº',
    #                'æº', 'æŠƒ', 'ï¿¼', 'ğŸ’«', '\U0001f9e1', 'éª', 'ğŸ·', 'è¥', 'ğŸ’²', 'æ¯', 'ğ“', 'ï¹€', 'è“¥', 'ğŸ”°', 'ğŸŒ™', 'ï¼´', 'é˜›',
    #                'äº¯', 'â–†', 'é‚´', 'æ–²', 'é€‹', 'ï¼¯', 'ğŸ˜»', 'ğŸ§', 'â•', 'ğŸ‚', 'ğŸ’™', 'ğŸˆ', 'â˜‘', 'ì˜', 'é”ƒ', 'ğŸš€', 'ğŸ•³', 'å¹¤',
    #                'ğŸƒ', 'é“§', 'ç¥¼', 'ğŸ‡º', 'ğŸŒ·', 'âŠ', 'è·¶', 'â˜Ÿ', 'â—', 'â‘«', 'ã€°', 'ç©', 'é²³', 'âˆ', 'çš', 'â”', '\uf06c', 'ë°•',
    #                'æ ¢', 'éªŸ', 'ğŸ˜', 'ğŸ‘ˆ', 'ğ™¯', 'é¿', 'æ§—', 'ç‹»', 'ì¤€', 'â¾ƒ', 'âœ', 'ğ™–', 'ğŸ‘‰', 'ğŸ½', 'Â¢', 'ë‚˜', 'ğŸ€', '\ue107',
    #                'ğŸ˜Œ', '\ue112', 'âœ¤', '\ue14c', 'ğŸ˜¡', 'ğŸ’', 'ğŸš¢', 'ğŸ‡¯', 'é˜', 'â˜', 'å§¤', 'ğŸµ', 'ğŸ’µ', 'â”‹', 'â„¡', 'ğŸ”œ',
    #                'ç', 'âœ˜', '\ue6fd', 'ğŸ†', 'â¼', 'ä¸…', 'æ¡„', 'â¾', 'ğŸ‘', 'æª©', 'å•­', 'ğ”', 'ã”', 'ï»ª', 'ç‡š', 'ğŸ˜Š', '\ue622',
    #                'ç‹¥', 'ç˜•', 'â', 'é’…', 'è—', 'å¨­', 'ğŸ‡©', 'â­', 'â“', 'â‹', 'ç‹´', 'ğŸŒ°', 'â¡', 'â‡™', 'ğŸ’‰', 'ğŸ‡³', 'ğŸ', 'å»‹', 'âƒ£',
    #                'â™', 'ğŸ‡§', 'ğŸ’', 'ç“', 'â‰ ', 'â¸', 'ğŸ˜', 'å—¾', 'èŠ˜', 'â½¤', 'ğŸŒ¼', 'ğŸ”Ÿ', 'â½£', 'èˆ¢', 'â–', 'ğŸ”', 'ğŸ’°', 'âŒ', 'æƒ¢',
    #                'é»‰', 'ğŸŒŸ', 'ğŸ”´', 'çŒ¢', 'ğŸ“ˆ', 'ğŸ‡', 'ğŸŠ', 'ğ™ª', 'ğ™©', 'â™‰', 'â—', 'â½‡', 'èŒ€', 'åœ‘', 'é–', 'ç‰–', 'å¯³', 'é²…',
    #                'ğŸ‡½', 'éŠ†', 'ç¶¯', 'ğŸ‰', 'ğŸ•¹', 'ğŸš„', 'âº', 'æ…±', 'â¼ˆ', 'æ¥’', 'ï¼£', 'ï¼§', 'ğŸŒ', 'â–£', 'â˜œ', 'ğŸ’´', '\ue246', 'ğ˜½',
    #                'ğŸŒˆ', 'å†š', 'ğŸ‡´', 'ğŸ‡­'
    #
    #                '\ue030', '\ue332', 'ç†»', '\ue022', '\ue60a', '\ue00e', '\ue608', 'çŠŸ', '\ue131', '\ue60e', '\ue609',
    #                'â”—', 'è¯“', '\ue219', '\ue607', 'èš¬', 'â”“', '\ue40b', '\ue10d', 'è®£', '\ue612', '\ue110', 'ç¼ƒ', 'é‘',
    #                '\ue611', '\ue60b', '\ue114', '\ue00d', 'è²¹', 'â”„', '\ue60f', 'â”', '\ue60c', 'æˆ£', 'åšš', 'â”›',
    #
    #                'â‘£', 'â€»', 'è«®', '$', 'ï½', 'ï¼™', '^', 'â—', 'å“¡', 'â–²', 'ï¼—', 'çˆ²', 'éšª', 'ï½', 'â”€', 'åœ’', 'ã¡', 'è²¨', 'â”‚', 'äº',
    #                'ç‡Ÿ', 'æ±', 'è¡“', 'å ±', 'é ', 'â’‹', 'â–³', 'è¬', 'å€‘', 'å»£', 'ç€', '*', 'å¾¬', 'è¤‡', 'é¢¨', 'ã®', 'åŒ¯', 'ï½', 'â‘ ', 'è¨ˆ',
    #                'é¦®', 'â‰¥', 'âˆš', 'éšŠ', 'ãƒ¼', 'ï¼•', 'ï¼˜', 'ï½–', '+', 'æ¨‚', 'å…§', 'åœ–', 'â‘¨', 'å…', 'è¼‰', 'è²¬', 'ç¢¼', 'çµ¶', 'æ€¼', 'èŠ',
    #                'å†', 'ç„¡', 'â˜†', 'è“‹', 'âˆ', 'é¡Œ', 'éŒ¯', 'ã€Œ', '{', 'â€–', 'ï¼…', 'è¦‹', 'â‘¥', 'è©', 'å¹£', 'ï¸±', 'é¬†', 'å•Ÿ', 'éš¨', 'â—‹',
    #                'ç¶“', 'é‘½', 'å¨›', 'æ¬Š', 'è¨Š', 'å­–', 'ç¾', 'è¶¨', 'å•', 'é', 'å€', 'é ', 'æ§‹', 'â•‘', 'åƒ±', 'ã€—', 'â†’', 'ç”¢', 'ï¼', 'è¦½',
    #                'ï¼', 'â’ˆ', 'å‘²', 'éˆ', 'â‘§', 'ã€', 'â†“', 'å›', 'èª°', 'çµ¡', 'æ©Ÿ', 'è³‡', 'åº«', 'ç‚º', 'çµ¦', 'ï¼–', 'ã€', 'â™‚', 'è¨­', 'æ¥­',
    #                'æ¨™', 'ï¼', 'é–“', 'è®Š', 'ã€•', 'é•·', '&', 'å¯¦', 'é€™', 'è«‡', 'â—‡', 'â‰§', 'å‰µ', 'æ©«', 'å¡—', "'", 'ã€', 'âˆ£', 'é€£', 'ç¸½',
    #                'ï½„', 'è³ ', 'åš', 'è¬€', 'â‰ˆ', 'â‘©', 'å°‡', '~', 'ç°¡', 'Î¸', 'é—œ', 'æœƒ', 'å€‰', 'æ±º', ']', 'ï¼', 'ã€ˆ', 'æˆ°', 'å‹¢', 'å¡Š',
    #                'æ°«', 'é€²', 'åƒ¹', 'è² ', 'æ½¤', 'æ™‚', 'â’‰', 'æ¼¢', 'æ“”', 'ï½', 'ç¨', 'ç¹«', '_', 'å‚³', 'éƒµ', 'æ²’', 'â–', 'è¯', 'â†—', 'ï¼‘',
    #                'æ“', 'é®®', 'é¨™', 'ã€', 'â—†', 'é›¢', 'ç´š', 'â–¡', 'ï¼', 'è¦–', 'é»ƒ', 'å°ˆ', 'é›»', 'æŒ', 'å ´', 'éš›', 'æ•¸', 'é›²', 'â– ', 'è±',
    #                'â˜…', 'è³ª', 'Â±', 'å‹™', 'â†‘', 'Â·', 'åƒ', 'ã€', 'æˆ€', 'è–…', 'ã€‰', 'é‰…', '"', 'ï¼”', 'ç’°', 'è©²', 'é–‹', 'â–Œ', 'â–¼', 'ã€–',
    #                'æ–·', '=', 'è­°', 'è³º', 'ä¾†', '\\', 'â‘¢', 'â’Š', 'å„Ÿ', 'è‰¹', 'èˆˆ', 'è©¢', 'é¦¬', 'ç¶²', 'ï¼', 'ï½’', '>', 'ï½‡', 'Ã·', 'ç¨…',
    #                'ã€”', 'è©³', 'å¸³', 'â‰¡', 'å„ª', 'ï¼‹', 'æ¸¬', 'â‘¡', 'å„²', 'è¯', 'è‰™', 'ï½™', 'ï¿¥', 'æ»¿', 'ï¼„', 'ï¹', 'éµ', 'â–‹', 'å» ', 'è²¡',
    #                'ï½œ', 'åœ‹', 'éŒ¢', 'æ†', 'ï½“', 'æˆ¶', '<', 'ï¼“', 'è²»', 'â‰¤', 'â€°', 'ç·Š', 'ï½‰', 'è±Š', 'é ‚', 'é«”', 'Â°', 'ï½', 'æ…®', 'é ˜',
    #                'â–ˆ', 'ï¼‚', 'çºŒ', 'é¾', 'é‚„', 'è½‰', 'è«‹', 'â†˜', 'â†', 'é¤˜', '}', 'Ã—', 'ç™¼', 'å¯©', '|', 'è¦', 'ç´…', 'åœ', 'â„ƒ', 'ï¼’',
    #                'å€‹', '@', 'å¼·', 'â‘¤', 'âŠ™', 'åœ˜', 'â‘¦',
    #
    #                'ç©©', 'èˆ‡', 'å±¤', 'ä¸¿', 'é‹', 'å‹•', 'Ï€', 'é”', 'ï¼œ', 'æº€', 'å¸¶', 'â”', 'æ¨“', 'å°', 'è³¼', 'â€²', 'å¯«', 'é¡', 'ç·š', 'æ²–',
    #                'é»', 'é‡', 'è´', 'ç¬', 'â–½', 'å¾', 'é …',
    #
    #                'Â©', 'æ­©', 'ï½•', 'é©Ÿ', 'â—', 'â™€', 'å¿ª', 'ç¸', 'è‡º', 'Ë‡', 'è£½', 'åˆ¥', 'é™¸', 'å¦³', 'â˜€', 'ãŠ£', 'æ…¶', 'ã€œ', 'ä¿¬', 'æ›',
    #                'â•°', 'âœ•', 'èº', 'è¼', 'â”Š', 'ï½…', 'èª¿', 'â€¢', 'æƒ¡', 'è­½', 'æ¢Ÿ', 'ã‚¢', 'ã‚ª', 'â”ƒ', 'ã€‡', 'çµ‚', 'â¤', 'è¼©', 'æ', 'å”',
    #                'ç©', 'Â®', 'é', 'ï½¤', 'è£¡', 'â–¶', 'â—¤', 'éˆ', 'éŠ€', 'à¹‘', 'ä¸Ÿ', 'è¬›', 'é »', 'æ»™', 'âœ–', 'è²³', 'ãƒ¦', 'ä½”', 'â€³', 'å¯¶',
    #                'Â»', 'â„¢', 'æ¸›', 'ï¼‡', 'â—¢', 'ç‘ª', 'è®“', 'ç¥‚', 'ğŸ˜‚', 'é©—', 'â‚¬', 'è¼”', 'âœ¦', 'Â¥', 'ï¹', 'ç´¡', 'å…’', 'ï¹‘', 'ã‚¡', 'ç…™',
    #                'â–‰', 'è¨¼', 'ï½', 'çµ', 'âœª', 'ï½›', 'Î²', 'ï½¡', 'ï¼»', 'èŠ', 'é½', 'â™¬', 'ç«¶', 'å„„', 'ï¼†', 'ã‚†', 'éŠ', 'ï¹¡', 'ç•¶', 'ï½ˆ',
    #                'è½„', 'ï½—', 'â¤', 'ã‚¦', 'æœ®', 'è™•', 'æ°£', 'è‡¨', 'å•²', 'ç‰ ', 'å•“', 'éŒ¦', 'è¡›', 'â–ª', 'æ¡', 'ã‚…', 'æµ', 'â™¥', 'æ·¨', 'ç¦¦',
    #                'â–ƒ', 'é¡˜', 'è¡', 'å‹', 'â˜', 'è¦º', 'è¨»', 'ï¼½', 'å•«', 'è˜­', 'æˆ²', 'â€', 'ã›', 'â–‚', 'é™³', 'å¢®', 'æ½°', 'âœ¨', 'Â«', 'æ¿Ÿ',
    #                'ç£¡', 'âœ”', 'ï¼Š', 'æ²¬',
    #
    #
    #                ]
    # for letter in letter_list:
    #   vocab[letter] = index
    #   index += 1
  return vocab


def convert_by_vocab(vocab, items):
  """Converts a sequence of [tokens|ids] using the vocab."""
  output = []
  for item in items:
    output.append(vocab[item])
  return output


def convert_tokens_to_ids(vocab, tokens):
  return convert_by_vocab(vocab, tokens)


def convert_ids_to_tokens(inv_vocab, ids):
  return convert_by_vocab(inv_vocab, ids)


def whitespace_tokenize(text):
  """Runs basic whitespace cleaning and splitting on a piece of text."""
  text = text.strip()
  if not text:
    return []
  tokens = text.split()
  return tokens


class FullTokenizer(object):
  """Runs end-to-end tokenziation."""

  def __init__(self, vocab_file, do_lower_case=True):
    self.vocab = load_vocab(vocab_file)
    self.inv_vocab = {v: k for k, v in self.vocab.items()}
    self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)
    self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)

  def tokenize(self, text):
    split_tokens = []
    for token in self.basic_tokenizer.tokenize(text):
      for sub_token in self.wordpiece_tokenizer.tokenize(token):
        split_tokens.append(sub_token)

    return split_tokens

  def convert_tokens_to_ids(self, tokens):
    return convert_by_vocab(self.vocab, tokens)

  def convert_ids_to_tokens(self, ids):
    return convert_by_vocab(self.inv_vocab, ids)


class BasicTokenizer(object):
  """Runs basic tokenization (punctuation splitting, lower casing, etc.)."""

  def __init__(self, do_lower_case=True):
    """Constructs a BasicTokenizer.

    Args:
      do_lower_case: Whether to lower case the input.
    """
    self.do_lower_case = do_lower_case

  def tokenize(self, text):
    """Tokenizes a piece of text."""
    text = convert_to_unicode(text)
    text = self._clean_text(text)

    # This was added on November 1st, 2018 for the multilingual and Chinese
    # models. This is also applied to the English models now, but it doesn't
    # matter since the English models were not trained on any Chinese data
    # and generally don't have any Chinese data in them (there are Chinese
    # characters in the vocabulary because Wikipedia does have some Chinese
    # words in the English Wikipedia.).
    text = self._tokenize_chinese_chars(text)

    orig_tokens = whitespace_tokenize(text)
    split_tokens = []
    for token in orig_tokens:
      if self.do_lower_case:
        token = token.lower()
        token = self._run_strip_accents(token)
      split_tokens.extend(self._run_split_on_punc(token))

    output_tokens = whitespace_tokenize(" ".join(split_tokens))
    return output_tokens

  def _run_strip_accents(self, text):
    """Strips accents from a piece of text."""
    text = unicodedata.normalize("NFD", text)
    output = []
    for char in text:
      cat = unicodedata.category(char)
      if cat == "Mn":
        continue
      output.append(char)
    return "".join(output)

  def _run_split_on_punc(self, text):
    """Splits punctuation on a piece of text."""
    chars = list(text)
    i = 0
    start_new_word = True
    output = []
    while i < len(chars):
      char = chars[i]
      if _is_punctuation(char):
        output.append([char])
        start_new_word = True
      else:
        if start_new_word:
          output.append([])
        start_new_word = False
        output[-1].append(char)
      i += 1

    return ["".join(x) for x in output]

  def _tokenize_chinese_chars(self, text):
    """Adds whitespace around any CJK character."""
    output = []
    for char in text:
      cp = ord(char)
      if self._is_chinese_char(cp):
        output.append(" ")
        output.append(char)
        output.append(" ")
      else:
        output.append(char)
    return "".join(output)

  def _is_chinese_char(self, cp):
    """Checks whether CP is the codepoint of a CJK character."""
    # This defines a "chinese character" as anything in the CJK Unicode block:
    #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)
    #
    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,
    # despite its name. The modern Korean Hangul alphabet is a different block,
    # as is Japanese Hiragana and Katakana. Those alphabets are used to write
    # space-separated words, so they are not treated specially and handled
    # like the all of the other languages.
    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #
        (cp >= 0x3400 and cp <= 0x4DBF) or  #
        (cp >= 0x20000 and cp <= 0x2A6DF) or  #
        (cp >= 0x2A700 and cp <= 0x2B73F) or  #
        (cp >= 0x2B740 and cp <= 0x2B81F) or  #
        (cp >= 0x2B820 and cp <= 0x2CEAF) or
        (cp >= 0xF900 and cp <= 0xFAFF) or  #
        (cp >= 0x2F800 and cp <= 0x2FA1F)):  #
      return True

    return False

  def _clean_text(self, text):
    """Performs invalid character removal and whitespace cleanup on text."""
    output = []
    for char in text:
      cp = ord(char)
      if cp == 0 or cp == 0xfffd or _is_control(char):
        continue
      if _is_whitespace(char):
        output.append(" ")
      else:
        output.append(char)
    return "".join(output)


class WordpieceTokenizer(object):
  """Runs WordPiece tokenziation."""

  def __init__(self, vocab, unk_token="[UNK]", max_input_chars_per_word=200):
    self.vocab = vocab
    self.unk_token = unk_token
    self.max_input_chars_per_word = max_input_chars_per_word

  def tokenize(self, text):
    """Tokenizes a piece of text into its word pieces.

    This uses a greedy longest-match-first algorithm to perform tokenization
    using the given vocabulary.

    For example:
      input = "unaffable"
      output = ["un", "##aff", "##able"]

    Args:
      text: A single token or whitespace separated tokens. This should have
        already been passed through `BasicTokenizer.

    Returns:
      A list of wordpiece tokens.
    """

    text = convert_to_unicode(text)

    output_tokens = []
    for token in whitespace_tokenize(text):
      chars = list(token)
      if len(chars) > self.max_input_chars_per_word:
        output_tokens.append(self.unk_token)
        continue

      is_bad = False
      start = 0
      sub_tokens = []
      while start < len(chars):
        end = len(chars)
        cur_substr = None
        while start < end:
          substr = "".join(chars[start:end])
          if start > 0:
            substr = "##" + substr
          if substr in self.vocab:
            cur_substr = substr
            break
          end -= 1
        if cur_substr is None:
          is_bad = True
          break
        sub_tokens.append(cur_substr)
        start = end

      if is_bad:
        output_tokens.append(self.unk_token)
      else:
        output_tokens.extend(sub_tokens)
    return output_tokens


def _is_whitespace(char):
  """Checks whether `chars` is a whitespace character."""
  # \t, \n, and \r are technically contorl characters but we treat them
  # as whitespace since they are generally considered as such.
  if char == " " or char == "\t" or char == "\n" or char == "\r":
    return True
  cat = unicodedata.category(char)
  if cat == "Zs":
    return True
  return False


def _is_control(char):
  """Checks whether `chars` is a control character."""
  # These are technically control characters but we count them as whitespace
  # characters.
  if char == "\t" or char == "\n" or char == "\r":
    return False
  cat = unicodedata.category(char)
  if cat in ("Cc", "Cf"):
    return True
  return False


def _is_punctuation(char):
  """Checks whether `chars` is a punctuation character."""
  cp = ord(char)
  # We treat all non-letter/number ASCII as punctuation.
  # Characters such as "^", "$", and "`" are not in the Unicode
  # Punctuation class but we treat them as punctuation anyways, for
  # consistency.
  if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or
      (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):
    return True
  cat = unicodedata.category(char)
  if cat.startswith("P"):
    return True
  return False
